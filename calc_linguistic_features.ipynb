{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "330a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "#from scipy.interpolate import make_interp_spline\n",
    "#from collections import Counter\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "essays = pd.read_csv(\"data/essays-without-markers.csv\", sep=\";\", encoding=\"UTF-8\")\n",
    "#essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38f6c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the titles from ChatGPT 4 essays\n",
    "essays['ChatGPT-4'] = essays['ChatGPT-4'].str.replace(r'Title:.*\\n\\r\\n', '', regex=True)\n",
    "\n",
    "# Preprocesses data with spaCy for later use\n",
    "\n",
    "essays[\"STUD_spacy\"] = essays[\"Student\"].apply(lambda x: nlp(x))\n",
    "essays[\"STUD_lemma\"] = essays[\"STUD_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))\n",
    "\n",
    "essays[\"GPT3_spacy\"] = essays[\"ChatGPT-3\"].apply(lambda x: nlp(x))\n",
    "essays[\"GPT3_lemma\"] = essays[\"GPT3_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))\n",
    "\n",
    "essays[\"GPT4_spacy\"] = essays[\"ChatGPT-4\"].apply(lambda x: nlp(x))\n",
    "essays[\"GPT4_lemma\"] = essays[\"GPT4_spacy\"].apply(lambda x: \" \".join([y.lemma_ for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bad396ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countes the number of sentences per essay\n",
    "\n",
    "def num_of_sent(text):\n",
    "    i = 0\n",
    "    for sentence in text.sents:\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "essays[\"STUD_sent_count\"] = essays[\"STUD_spacy\"].apply(lambda x: num_of_sent(x))\n",
    "essays[\"GPT3_sent_count\"] = essays[\"GPT3_spacy\"].apply(lambda x: num_of_sent(x))\n",
    "essays[\"GPT4_sent_count\"] = essays[\"GPT4_spacy\"].apply(lambda x: num_of_sent(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34021711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countes the number of words per essay\n",
    "\n",
    "def num_of_words(text):\n",
    "    count = len(text.split())\n",
    "    return count\n",
    "\n",
    "essays[\"STUD_word_count\"] = essays[\"Student\"].apply(lambda x: num_of_words(x))\n",
    "essays[\"GPT3_word_count\"] = essays[\"ChatGPT-3\"].apply(lambda x: num_of_words(x))\n",
    "essays[\"GPT4_word_count\"] = essays[\"ChatGPT-4\"].apply(lambda x: num_of_words(x))       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c9782",
   "metadata": {},
   "source": [
    "### Sentence complexity\n",
    "\n",
    "We have two score for sentence complexity. \n",
    "\n",
    "1. One based on a number of particular dependnecy labels found in each sentence (Clausal modifier of noun; Conjunct; Adverbial clause modifier; Clausal complement; Clausal subject; Discourse; Parataxis)\n",
    "2. Second one is based on the depth of the dependency tree\n",
    "\n",
    "The output values are mean values of the number of the tags per sentence and of the depth of the dependency tree of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54c97b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualtes the number of specified dependency label within a sentence\n",
    "def calculate_dep_score(text):\n",
    "    temp = []\n",
    "    for sentence in nlp(text).sents:\n",
    "        temp.append(sent_complexity_structure(sentence))\n",
    "    return np.mean(temp)\n",
    "\n",
    "# Return the number of specified dependency labels found\n",
    "def sent_complexity_structure(doc):\n",
    "    return len([token for token in doc if (token.dep_ == \"acl\" or token.dep_ == \"conj\" or token.dep_ == \"advcl\"or token.dep_ == \"ccomp\"\n",
    "    or token.dep_ == \"csubj\" or token.dep_ == \"discourse\" or token.dep_ == \"parataxis\")])\n",
    "\n",
    "# Calculates the dependency depth \n",
    "def calculate_dep_length(text):\n",
    "    temp = []\n",
    "    for sentence in nlp(text).sents:\n",
    "        temp.append(walk_tree(sentence.root, 0))\n",
    "    return np.mean(temp)  \n",
    "\n",
    "# Walks the dependency tree and returns the depth\n",
    "def walk_tree(node, depth):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return max(walk_tree(child, depth + 1) for child in node.children)\n",
    "    else:\n",
    "        return depth\n",
    "\n",
    "\n",
    "essays[\"STUD_sent_complex_tags\"] = essays[\"Student\"].apply(lambda x: calculate_dep_score(x))\n",
    "essays[\"STUD_sent_complex_depth\"] = essays[\"Student\"].apply(lambda x: calculate_dep_length(x))\n",
    "\n",
    "essays[\"GPT3_sent_complex_tags\"] = essays[\"ChatGPT-3\"].apply(lambda x: calculate_dep_score(x))\n",
    "essays[\"GPT3_sent_complex_depth\"] = essays[\"ChatGPT-3\"].apply(lambda x: calculate_dep_length(x))\n",
    "\n",
    "essays[\"GPT4_sent_complex_tags\"] = essays[\"ChatGPT-4\"].apply(lambda x: calculate_dep_score(x))\n",
    "essays[\"GPT4_sent_complex_depth\"] = essays[\"ChatGPT-4\"].apply(lambda x: calculate_dep_length(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d40987c",
   "metadata": {},
   "source": [
    "### Lexical diversity\n",
    "\n",
    "Calculating lexical diverstity score using MTLD measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "755aa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates MTLD score for the whole essay\n",
    "\n",
    "def calculate_lex_richness_MTLD2(text):\n",
    "    lex = LexicalRichness(text) \n",
    "    lex_rich_score = lex.mtld()\n",
    "    return(lex_rich_score)\n",
    "\n",
    "essays[\"STUD_LD\"] = essays[\"Student\"].apply(lambda x: calculate_lex_richness_MTLD2(x))\n",
    "essays[\"GPT3_LD\"] = essays[\"ChatGPT-3\"].apply(lambda x: calculate_lex_richness_MTLD2(x))\n",
    "essays[\"GPT4_LD\"] = essays[\"ChatGPT-4\"].apply(lambda x: calculate_lex_richness_MTLD2(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4e2f7",
   "metadata": {},
   "source": [
    "### Discourse markers\n",
    "\n",
    "Calculating number of discourse markers from Penn Discourse Tree Bank per essay (some discourse markers (about, as, by, both, for, from, given, in, like, on, once, only, still, when, with, without, yet, and) we excluded from the list because they can often be used as not discourse markers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd52af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of discourse markers using PDTB list\n",
    "\n",
    "discourse = pd.read_csv(\"markers/connectives_discourse_markers_PDTB.txt\", sep=\"\\'\", encoding=\"UTF-8\", header=None, usecols = [1,3])\n",
    "\n",
    "discourse[3] = discourse[3].apply(lambda x: x.replace(\"t_conn_\", \"\"))\n",
    "discourse[1] = discourse[1].apply(lambda x: \" \" + x + \" \")\n",
    "discourse.sort_values(3, inplace=True, ascending=False)\n",
    "\n",
    "# Countes the total numbers of discourse markers per essay\n",
    "def count_discourse_markers(text):\n",
    "    i = 0\n",
    "    for marker in discourse.itertuples():\n",
    "        if marker[1] in text:\n",
    "            i += text.count(marker[1])\n",
    "    return i\n",
    "\n",
    "essays[\"STUD_discourse\"] = essays[\"STUD_lemma\"].apply(lambda x: count_discourse_markers(x))\n",
    "essays[\"GPT3_discourse\"] = essays[\"GPT3_lemma\"].apply(lambda x: count_discourse_markers(x))\n",
    "essays[\"GPT4_discourse\"] = essays[\"GPT4_lemma\"].apply(lambda x: count_discourse_markers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cfa5d",
   "metadata": {},
   "source": [
    "### Modals\n",
    "\n",
    "Counting the number of modals using POS tag \"MD\" and the modals.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "064aa75a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counts the number of modals from the list of modals\n",
    "\n",
    "modals = pd.read_csv(\"markers/modals.csv\", sep=\",\", encoding=\"UTF-8\", header=None)\n",
    "modals[0] = modals[0].apply(lambda x: x.replace('_', ' '))\n",
    "\n",
    "# Counts the number of modals per essay\n",
    "def count_total_modals(text):\n",
    "    counter = 0\n",
    "    for modal in modals.itertuples():\n",
    "        if modal[1] in text:\n",
    "            counter += text.count(modal[1])\n",
    "    return counter\n",
    "\n",
    "essays[\"STUD_modals1\"] = essays[\"STUD_lemma\"].apply(lambda x: count_total_modals(x))\n",
    "essays[\"GPT3_modals1\"] = essays[\"GPT3_lemma\"].apply(lambda x: count_total_modals(x))\n",
    "essays[\"GPT4_modals1\"] = essays[\"GPT4_lemma\"].apply(lambda x: count_total_modals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1814edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of modals using POS tagging\n",
    "\n",
    "essays[\"STUD_pos\"] = essays[\"STUD_spacy\"].apply(lambda x: \" \".join([y.tag_ for y in x]))\n",
    "essays[\"GPT3_pos\"] = essays[\"GPT3_spacy\"].apply(lambda x: \" \".join([y.tag_ for y in x]))\n",
    "essays[\"GPT4_pos\"] = essays[\"GPT4_spacy\"].apply(lambda x: \" \".join([y.tag_ for y in x]))\n",
    "\n",
    "essays[\"STUD_modals2\"] = essays[\"STUD_pos\"].str.count(r'MD')\n",
    "essays[\"GPT3_modals2\"] = essays[\"GPT3_pos\"].str.count(r'MD')\n",
    "essays[\"GPT4_modals2\"] = essays[\"GPT4_pos\"].str.count(r'MD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c1132c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates total number of modals per essay\n",
    "\n",
    "essays[\"STUD_modals_all\"] = essays[\"STUD_modals2\"] + essays[\"STUD_modals1\"]\n",
    "essays[\"GPT3_modals_all\"] = essays[\"GPT3_modals2\"] + essays[\"GPT3_modals1\"]\n",
    "essays[\"GPT4_modals_all\"] = essays[\"GPT4_modals2\"] + essays[\"GPT4_modals1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba67d1",
   "metadata": {},
   "source": [
    "### Epistemic markers\n",
    " \n",
    "Getting the number of epistemic markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2758a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the total number of epistemic markers per essay\n",
    "\n",
    "def find_epistemic_markers(text):\n",
    "    ep_markers = []\n",
    "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)(?:\\s\\w+)?(?:\\s\\w+)?\\s(?:believes?|thinks?|means?|worry|worries|know|guesse?s?|assumes?)\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"(?:It|it)\\sis\\s(?:believed|known|assumed|thought)\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"(?:I|We|we)\\s(?:am|are)\\s(?:thinking|guessing)\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)(?:\\s\\w+)?\\s(?:do|does)\\snot\\s(?:believe?|think|know)\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)\\swould(?:\\s\\w+)?(?:\\snot)?\\ssay\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"I\\sam\\s(?:afraid|sure|confident)\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"(?:My|my|Our|our)\\s(?:experience|opinion|belief|knowledge|worry|worries|concerns?|guesse?s?)\\s(?:is|are)\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"[In]n\\s(?:my|our)(?:\\s\\w+)?\\sopinion\", text))\n",
    "    ep_markers.extend(re.findall(r\"As\\sfar\\sas\\s(?:I|We|we)\\s(?:am|are)\\sconcerned\", text))\n",
    "    ep_markers.extend(re.findall(r\"(?:I|We|we|One|one)\\s(?:can|could|may|might)(?:\\s\\w+)?\\sconclude\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"I\\s(?:am\\swilling\\sto|must)\\ssay\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"One\\s(?:can|could|may|might)\\ssay\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"[Oo]ne\\s(?:can|could|may|might)\\ssay\\s(?:that)?\", text))\n",
    "    ep_markers.extend(re.findall(r\"[Ii]t\\sis\\s(?:obvious|(?:un)?clear)\", text))\n",
    "    ep_markers.extend(re.findall(r\"[Ii]t\\s(?:seems|feels|looks)\", text))\n",
    "    return len(ep_markers)\n",
    "\n",
    "essays[\"STUD_EpMarkers\"] = essays[\"Student\"].apply(lambda x: find_epistemic_markers(x))\n",
    "essays[\"GPT3_EpMarkers\"] = essays[\"ChatGPT-3\"].apply(lambda x: find_epistemic_markers(x))\n",
    "essays[\"GPT4_EpMarkers\"] = essays[\"ChatGPT-4\"].apply(lambda x: find_epistemic_markers(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60af5d",
   "metadata": {},
   "source": [
    "### Nominalisations\n",
    "\n",
    "Counting the number of nominalisations per essay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12205383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the total number of nominalisations per essay\n",
    "\n",
    "def nominalisation_counter(text):\n",
    "    suffixes_n = r'\\b[A-Z]*\\w+(?:tion|ment|ance|ence|ion|it(?:y|ies)|ness|ship)(?:s|es)?\\b'\n",
    "    \n",
    "    nom_nouns = []    \n",
    "    nouns = [token.text for token in text if token.pos_ == 'NOUN']  \n",
    "    nom_nouns = [noun for noun in nouns if re.match(suffixes_n, noun)] \n",
    "    \n",
    "    return(len(nom_nouns))\n",
    "    \n",
    "essays[\"STUD_nominalisation\"] = essays[\"STUD_spacy\"].apply(lambda x: nominalisation_counter(x))\n",
    "essays[\"GPT3_nominalisation\"] = essays[\"GPT3_spacy\"].apply(lambda x: nominalisation_counter(x))\n",
    "essays[\"GPT4_nominalisation\"] = essays[\"GPT4_spacy\"].apply(lambda x: nominalisation_counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5935b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the average number of features (discourse markers, modals, epistemic markers, nominalisations) per sentence for each essay\n",
    "\n",
    "def average_per_sentence(feature, sent):\n",
    "    average = feature/sent\n",
    "    return(average)\n",
    "\n",
    "essays[\"STUD_dm_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"STUD_discourse\"], row[\"STUD_sent_count\"]), axis=1)\n",
    "essays[\"GPT3_dm_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT3_discourse\"], row[\"GPT3_sent_count\"]), axis=1)\n",
    "essays[\"GPT4_dm_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT4_discourse\"], row[\"GPT4_sent_count\"]), axis=1)\n",
    "\n",
    "essays[\"STUD_mod_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"STUD_modals_all\"], row[\"STUD_sent_count\"]), axis=1)\n",
    "essays[\"GPT3_mod_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT3_modals_all\"], row[\"GPT3_sent_count\"]), axis=1)\n",
    "essays[\"GPT4_mod_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT4_modals_all\"], row[\"GPT4_sent_count\"]), axis=1)\n",
    "\n",
    "essays[\"STUD_ep_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"STUD_EpMarkers\"], row[\"STUD_sent_count\"]), axis=1)\n",
    "essays[\"GPT3_ep_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT3_EpMarkers\"], row[\"GPT3_sent_count\"]), axis=1)\n",
    "essays[\"GPT4_ep_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT4_EpMarkers\"], row[\"GPT4_sent_count\"]), axis=1)\n",
    "\n",
    "essays[\"STUD_nom_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"STUD_nominalisation\"], row[\"STUD_sent_count\"]), axis=1)\n",
    "essays[\"GPT3_nom_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT3_nominalisation\"], row[\"GPT3_sent_count\"]), axis=1)\n",
    "essays[\"GPT4_nom_per_sent\"] = essays.apply(lambda row: average_per_sentence(row[\"GPT4_nominalisation\"], row[\"GPT4_sent_count\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "240f35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence complexity based on a number of certain dependency tags\n",
      "Student: 1.8079994478858226\n",
      "GPT3:  2.305861562185092\n",
      "GPT4:  2.079634224707754 \n",
      "\n",
      "Sentence complexity based on the tree depth\n",
      "Student: 5.722130126604919\n",
      "GPT3:  6.181905400336772\n",
      "GPT4:  5.937846240542318 \n",
      "\n",
      "MTLD lexical diversity score\n",
      "GPT4: 108.90580635929804\n",
      "GPT3:  75.67987401417784\n",
      "Student:  95.72135147300236 \n",
      "\n",
      "Average number of discourse markers per essay\n",
      "Student: 9.666666666666666\n",
      "GPT3:  6.277777777777778\n",
      "GPT4:  4.722222222222222 \n",
      "\n",
      "Average number of modals (from the list) per essay\n",
      "Student: 2.1555555555555554\n",
      "GPT3:  1.4555555555555555\n",
      "GPT4:  1.6 \n",
      "\n",
      "Average number of modals (POS-tags) per essay\n",
      "Student: 8.688888888888888\n",
      "GPT3:  7.511111111111111\n",
      "GPT4:  4.522222222222222 \n",
      "\n",
      "Average number of epistemic markers per essay\n",
      "Student: 1.0222222222222221\n",
      "GPT3:  0.2\n",
      "GPT4:  0.0 \n",
      "\n",
      "Average number of nominalisations per essay\n",
      "Student: 16.91111111111111\n",
      "GPT3:  18.955555555555556\n",
      "GPT4:  22.355555555555554\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence complexity based on a number of certain dependency tags\")\n",
    "print(\"Student:\", np.mean(essays[\"STUD_sent_complex_tags\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_sent_complex_tags\"]))\n",
    "print(\"GPT4: \", np.mean(essays[\"GPT4_sent_complex_tags\"]), \"\\n\")\n",
    "\n",
    "print(\"Sentence complexity based on the tree depth\")\n",
    "print(\"Student:\", np.mean(essays[\"STUD_sent_complex_depth\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_sent_complex_depth\"]))\n",
    "print(\"GPT4: \", np.mean(essays[\"GPT4_sent_complex_depth\"]), \"\\n\")\n",
    "\n",
    "print(\"MTLD lexical diversity score\")\n",
    "print(\"GPT4:\", np.mean(essays[\"GPT4_LD\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_LD\"]))\n",
    "print(\"Student: \", np.mean(essays[\"STUD_LD\"]), \"\\n\")\n",
    "\n",
    "print(\"Average number of discourse markers per essay\")\n",
    "print(\"Student:\", np.mean(essays[\"STUD_discourse\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_discourse\"]))\n",
    "print(\"GPT4: \", np.mean(essays[\"GPT4_discourse\"]), \"\\n\")\n",
    "\n",
    "print(\"Average number of modals (from the list) per essay\")\n",
    "print(\"Student:\", np.mean(essays[\"STUD_modals1\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_modals1\"]))\n",
    "print(\"GPT4: \", np.mean(essays[\"GPT4_modals1\"]), \"\\n\")\n",
    "\n",
    "print(\"Average number of modals (POS-tags) per essay\")\n",
    "print(\"Student:\", np.mean(essays[\"STUD_modals2\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_modals2\"]))\n",
    "print(\"GPT4: \", np.mean(essays[\"GPT4_modals2\"]), \"\\n\")\n",
    "\n",
    "print(\"Average number of epistemic markers per essay\")\n",
    "print(\"Student:\", np.mean(essays[\"STUD_EpMarkers\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_EpMarkers\"]))\n",
    "print(\"GPT4: \", np.mean(essays[\"GPT4_EpMarkers\"]), \"\\n\")\n",
    "\n",
    "print(\"Average number of nominalisations per essay\")\n",
    "print(\"Student:\", np.mean(essays[\"STUD_nominalisation\"]))\n",
    "print(\"GPT3: \", np.mean(essays[\"GPT3_nominalisation\"]))\n",
    "print(\"GPT4: \", np.mean(essays[\"GPT4_nominalisation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb4ca0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "essays.to_csv(\"data/essays-with-linguistic-markers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "657977a8e1cca15745b845f6b06b6c4354282a12f318ae778563a98fa73562e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
